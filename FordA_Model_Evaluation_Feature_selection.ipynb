{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ford A Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = pd.read_csv('C:/Users/user/Desktop/AI & ML/Project/X_train.csv',index_col='file_name')\n",
    "df_featuret = pd.read_csv('C:/Users/user/Desktop/AI & ML/Project/X_test.csv',index_col='file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "sensitivity_score = []\n",
    "specificity_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_feature = shuffle(df_feature)\n",
    "\n",
    "X_train = df_feature.drop(['label'],axis=1)\n",
    "y_train = df_feature['label']\n",
    "X_test = df_featuret.drop(['label'],axis=1)\n",
    "y_test = df_featuret['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_METRIC = 'entropy'  #metric used for RandomForrestClassifier\n",
    "NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\n",
    "NO_JOBS = -1 #number of parallel jobs used for RandomForrestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                             random_state=2000,\n",
    "                             criterion=RFC_METRIC,\n",
    "                             n_estimators=NUM_ESTIMATORS,\n",
    "                             verbose=False)\n",
    "\n",
    "scores_RF = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "scores_RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "pred1 = clf.predict(X_test)\n",
    "acc_score_RF = accuracy_score(y_test,pred1)\n",
    "acc_score_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred1)\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "Sensitivity_RF = TP / (TP + FN)\n",
    "Specificity_RF = TN / (TN + FP)\n",
    "\n",
    "print(F\"Sensitivity is { Sensitivity_RF }\")\n",
    "print(F\"Specificity is { Specificity_RF }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.append(acc_score_RF)\n",
    "sensitivity_score.append(Sensitivity_RF)\n",
    "specificity_score.append(Specificity_RF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100], \n",
    "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                'kernel': ['linear','rbf']} \n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3) \n",
    "\n",
    "# fitting the model for grid search \n",
    "grid.fit(X_train, y_train) \n",
    "\n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "\n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(grid.best_estimator_) \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm1 = svm.SVC(kernel='linear', C=1)\n",
    "scores_svm1 = cross_val_score(clf_svm1, X_train, y_train, cv=5)\n",
    "scores_svm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm1.fit(X_train,y_train)\n",
    "pred2 = clf_svm1.predict(X_test)\n",
    "acc_score_svm1 = accuracy_score(y_test,pred2)\n",
    "acc_score_svm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred2)\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "Sensitivity_svm1 = TP / (TP + FN)\n",
    "Specificity_svm1 = TN / (TN + FP)\n",
    "\n",
    "print(F\"Sensitivity is { Sensitivity_svm1 }\")\n",
    "print(F\"Specificity is { Specificity_svm1 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.append(acc_score_svm1)\n",
    "sensitivity_score.append(Sensitivity_svm1)\n",
    "specificity_score.append(Specificity_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm2 = svm.SVC(kernel='rbf', C=1)\n",
    "scores_svm2 = cross_val_score(clf_svm2, X_train, y_train, cv=5)\n",
    "scores_svm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm2.fit(X_train,y_train)\n",
    "pred2 = clf_svm2.predict(X_test)\n",
    "acc_score_svm2 = accuracy_score(y_test,pred2)\n",
    "acc_score_svm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred2)\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "Sensitivity_svm2 = TP / (TP + FN)\n",
    "Specificity_svm2 = TN / (TN + FP)\n",
    "\n",
    "print(F\"Sensitivity is { Sensitivity_svm2 }\")\n",
    "print(F\"Specificity is { Specificity_svm2 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.append(acc_score_svm2)\n",
    "sensitivity_score.append(Sensitivity_svm2)\n",
    "specificity_score.append(Specificity_svm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm3 = svm.SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "scores_svm3 = cross_val_score(clf_svm3, X_train, y_train, cv=5)\n",
    "scores_svm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm3.fit(X_train,y_train)\n",
    "pred3 = clf_svm3.predict(X_test)\n",
    "acc_score_svm3 = accuracy_score(y_test,pred3)\n",
    "acc_score_svm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred2)\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "Sensitivity_svm3 = TP / (TP + FN)\n",
    "Specificity_svm3 = TN / (TN + FP)\n",
    "\n",
    "print(F\"Sensitivity is { Sensitivity_svm3 }\")\n",
    "print(F\"Specificity is { Specificity_svm3 }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.append(acc_score_svm3)\n",
    "sensitivity_score.append(Sensitivity_svm3)\n",
    "specificity_score.append(Specificity_svm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('rf', clf))\n",
    "estimators.append(('svm', clf_svm1))\n",
    "estimators.append(('svm3', clf_svm3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ens = cross_val_score(ensemble, X_train, y_train, cv=5)\n",
    "scores_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train,y_train)\n",
    "pred4 = ensemble.predict(X_test)\n",
    "acc_score_ens = accuracy_score(y_test,pred4)\n",
    "acc_score_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,pred3)\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "Sensitivity_ens = TP / (TP + FN)\n",
    "Specificity_ens = TN / (TN + FP)\n",
    "\n",
    "print(F\"Sensitivity is { Sensitivity_ens }\")\n",
    "print(F\"Specificity is { Specificity_ens }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.append(acc_score_ens)\n",
    "sensitivity_score.append(Sensitivity_ens)\n",
    "specificity_score.append(Specificity_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig= plt.figure(figsize=(18,3))\n",
    "plt.plot( [1,2,3,4,5], scores_RF, linewidth=2,label=\"RF\")\n",
    "plt.plot( [1,2,3,4,5], scores_svm1, linewidth=2,label=\"SVM1\")\n",
    "plt.plot( [1,2,3,4,5], scores_svm2, linewidth=2,label=\"SVM2\")\n",
    "plt.plot( [1,2,3,4,5], scores_svm3, linewidth=2,label=\"SVM3\")\n",
    "plt.plot( [1,2,3,4,5], scores_ens, linewidth=2,label=\"ENS\")\n",
    "plt.legend()\n",
    "plt.title(\"KFold trend of different Models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracy_listnp = np.array(accuracy_list)\n",
    "accuracy_listnp = accuracy_listnp * 100\n",
    "\n",
    "fig= plt.figure(figsize=(9,6))\n",
    "plt.ylim( (0, 100) ) \n",
    "plt.bar([1,2,3,4,5],accuracy_listnp,color=['orange', 'red', 'green', 'blue', 'cyan'])\n",
    "plt.xticks([1,2,3,4,5], ('RF', 'SVM1', 'SVM2', 'SVM3', 'ENSEMBLE'))\n",
    "plt.title(\"Bar plot of accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12,6))\n",
    "\n",
    "# data to plot\n",
    "n_groups = 5\n",
    "sensitivity_scorenp = np.array(sensitivity_score)\n",
    "sensitivity_scorenp = sensitivity_scorenp * 100\n",
    "specificity_scorenp = np.array(specificity_score)\n",
    "specificity_scorenp = specificity_scorenp * 100\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, sensitivity_scorenp, bar_width,\n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='Sensitivity')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, specificity_scorenp, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='specificity')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Sensitivity and Specificity')\n",
    "plt.xticks(index + bar_width, ('RF', 'SVM1', 'SVM2', 'SVM3', 'ENSEMBLE'))\n",
    "plt.ylim(0,100)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrmr_feature = [['mcr_cd1', 'zcr_ca4', 'ac499', 'mcr_ca4', 'mcr_fft', 'mean_psd', 'var_ca3', 'kurt_psd', 'ac498', 'skew_fftw', 'ac483', 'CF_fft', 'ac478', 'ac473', 'ac1', 'ac461', 'ac306', 'ac2', 'rms_ca3', 'zcr_ca3', 'CF_ca3', 'mcr_ca3', 'mean_ca4', 'std_ca4', 'CF_ca2', 'rms_ca2', 'std_ca2', 'skew_ca2', 'kurt_ca2', 'mode_ca2', 'mode_ca3', 'kurt_ca3', 'zcr_ca2', 'mean_ca3', 'std_ca3', 'skew_ca3', 'var_ca4', 'mean_ca2', 'ac489', 'ac491', 'ac486', 'ac488', 'ac479', 'ac490', 'ac465', 'ac496', 'ac497', 'ac481', 'skew_ca4', 'ac467'],['mcr_cd1', 'zcr_ca4', 'ac499', 'mcr_ca4', 'mcr_fft', 'mean_psd', 'var_ca3', 'kurt_psd', 'ac498', 'skew_fftw', 'ac483', 'CF_fft', 'ac478', 'ac473', 'ac1', 'ac461', 'ac306', 'ac2', 'rms_ca3', 'zcr_ca3', 'CF_ca3', 'mcr_ca3', 'mean_ca4', 'std_ca4', 'CF_ca2', 'rms_ca2', 'std_ca2', 'skew_ca2', 'kurt_ca2', 'mode_ca2', 'mode_ca3', 'kurt_ca3', 'zcr_ca2', 'mean_ca3', 'std_ca3', 'skew_ca3', 'var_ca4', 'mean_ca2', 'ac489', 'ac491'],['mcr_cd1', 'zcr_ca4', 'ac499', 'mcr_ca4', 'mcr_fft', 'mean_psd', 'var_ca3', 'kurt_psd', 'ac498', 'skew_fftw', 'ac483', 'CF_fft', 'ac478', 'ac473', 'ac1', 'ac461', 'ac306', 'ac2', 'rms_ca3', 'zcr_ca3', 'CF_ca3', 'mcr_ca3', 'mean_ca4', 'std_ca4', 'CF_ca2', 'rms_ca2', 'std_ca2', 'skew_ca2', 'kurt_ca2', 'mode_ca2'],['mcr_cd1', 'zcr_ca4', 'ac499', 'mcr_ca4', 'mcr_fft', 'mean_psd', 'var_ca3', 'kurt_psd', 'ac498', 'skew_fftw', 'ac483', 'CF_fft', 'ac478', 'ac473', 'ac1', 'ac461', 'ac306', 'ac2', 'rms_ca3', 'zcr_ca3'],['mcr_cd1', 'zcr_ca4', 'ac499', 'mcr_ca4', 'mcr_fft', 'mean_psd', 'var_ca3', 'kurt_psd', 'ac498', 'skew_fftw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = []\n",
    "test_score = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "i = 0\n",
    "for fea_set in mrmr_feature:\n",
    "    print(\"*************************************************\")\n",
    "    print(F\"features in use - \\n { fea_set }\")\n",
    "    try:\n",
    "        del X_train,clfrf,clf_svm1_lin,clf_svm_rbf,ensemble\n",
    "    except NameError as e:\n",
    "        pass\n",
    "    X_train = df_feature[fea_set]\n",
    "    y_train = df_feature['label']\n",
    "    X_test = df_featuret[fea_set]\n",
    "    y_test = df_featuret['label']\n",
    "    #scale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    #RF\n",
    "    RFC_METRIC = 'entropy'  #metric used for RandomForrestClassifier\n",
    "    NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\n",
    "    NO_JOBS = -1 #number of parallel jobs used for RandomForrestClassifier\n",
    "    clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                                 random_state=2000,\n",
    "                                 criterion=RFC_METRIC,\n",
    "                                 n_estimators=NUM_ESTIMATORS,\n",
    "                                 verbose=False)\n",
    "\n",
    "    scores_rf = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_rf)\n",
    "    print(F\"training score for RF is - { scores_rf }\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred1 = clf.predict(X_test)\n",
    "    acc_score_rf = accuracy_score(y_test,pred1)\n",
    "    test_score.append(acc_score_rf)\n",
    "    print(F\"test acc_score for RF is - { acc_score_rf }\")\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred1)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_rf = TP / (TP + FN)\n",
    "    Specificity_rf = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for RF is { Sensitivity_rf }\")\n",
    "    print(F\"Specificity for RF is { Specificity_rf }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_rf)\n",
    "    specificity.append(Specificity_rf)\n",
    "    \n",
    "    #SVM linear\n",
    "    \n",
    "    clf_svm1 = svm.SVC(kernel='linear', C=1)\n",
    "    scores_svm1 = cross_val_score(clf_svm1, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_svm1)\n",
    "    print(F\"training score for svm linear is - { scores_svm1 }\")\n",
    "    clf_svm1.fit(X_train,y_train)\n",
    "    pred2 = clf_svm1.predict(X_test)\n",
    "    acc_score_svm1 = accuracy_score(y_test,pred2)\n",
    "    print(F\"test acc_score for svm linear is - { acc_score_svm1 }\")\n",
    "    test_score.append(acc_score_svm1)\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred2)\n",
    "    TP = cm[0][0] \n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm1 = TP / (TP + FN)\n",
    "    Specificity_svm1 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm linear is { Sensitivity_svm1 }\")\n",
    "    print(F\"Specificity for svm linear is { Specificity_svm1 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm1)\n",
    "    specificity.append(Specificity_svm1)\n",
    "    \n",
    "    #SVM rbf\n",
    "    clf_svm2 = svm.SVC(kernel='rbf', C=1)\n",
    "    scores_svm2 = cross_val_score(clf_svm2, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_svm2)\n",
    "    print(F\"training score for svm rbf is - { scores_svm2 }\")\n",
    "    clf_svm2.fit(X_train,y_train)\n",
    "    pred3 = clf_svm2.predict(X_test)\n",
    "    acc_score_svm2 = accuracy_score(y_test,pred3)\n",
    "    print(F\"test acc_score for svm rbf is - { acc_score_svm2 }\")\n",
    "    test_score.append(acc_score_svm2)\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred3)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm2 = TP / (TP + FN)\n",
    "    Specificity_svm2 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm rbf is { Sensitivity_svm2 }\")\n",
    "    print(F\"Specificity for svm rbf is { Specificity_svm2 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm2)\n",
    "    specificity.append(Specificity_svm2)\n",
    "    #svm3\n",
    "    clf_svm3 = svm.SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "      decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
    "      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "      tol=0.001, verbose=False)\n",
    "    scores_svm3 = cross_val_score(clf_svm3, X_train, y_train, cv=5)\n",
    "    print(F\"training score for svm rbf is - { scores_svm3 }\")\n",
    "    training_score.append(scores_svm3)\n",
    "    clf_svm3.fit(X_train,y_train)\n",
    "    pred4 = clf_svm3.predict(X_test)\n",
    "    acc_score_svm3 = accuracy_score(y_test,pred4)\n",
    "    print(F\"test acc_score for svm rbf is - { acc_score_svm3 }\")\n",
    "    test_score.append(acc_score_svm3)\n",
    "    cm = confusion_matrix(y_test,pred4)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm3 = TP / (TP + FN)\n",
    "    Specificity_svm3 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm rbf is { Sensitivity_svm3 }\")\n",
    "    print(F\"Specificity for svm rbf is { Specificity_svm3 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm3)\n",
    "    specificity.append(Specificity_svm3)\n",
    "    #ensemble\n",
    "    estimators = []\n",
    "    estimators.append(('rf', clf))\n",
    "    estimators.append(('svm', clf_svm1))\n",
    "    estimators.append(('svm3', clf_svm3))\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    scores_ens = cross_val_score(ensemble, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_ens)\n",
    "    print(F\"score for eensemble is - { scores_ens }\")\n",
    "    ensemble.fit(X_train,y_train)\n",
    "    pred4 = ensemble.predict(X_test)\n",
    "    acc_score_ens = accuracy_score(y_test,pred4)   \n",
    "    print(F\"acc_score for ensemble is - { acc_score_ens }\")\n",
    "    test_score.append(acc_score_ens)\n",
    "    cm = confusion_matrix(y_test,pred4)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_ens = TP / (TP + FN)\n",
    "    Specificity_ens = TN / (TN + FP)\n",
    "    sensitivity.append(Sensitivity_ens)\n",
    "    specificity.append(Specificity_ens)\n",
    "\n",
    "    print(F\"Sensitivity for ensemble is { Sensitivity_ens }\")\n",
    "    print(F\"Specificity for ensemble is { Specificity_ens }\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination\n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = shuffle(df_feature)\n",
    "\n",
    "X_train_full = df_feature.drop(['label'],axis=1)\n",
    "all_features = X_train_full.columns\n",
    "y_train_full = df_feature['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full)\n",
    "X_train_full = scaler.transform(X_train_full)\n",
    "y_train_full = np.array(y_train_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_rfe = svm.SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "for ix in [20,30,40,50,60]:\n",
    "    rfe = RFE(clf_svm_rfe, ix)\n",
    "    fit = rfe.fit(X_train_full,y_train_full)\n",
    "    best_fea = fit.ranking_\n",
    "    index = []\n",
    "    for i in range(0,len(best_fea)):\n",
    "        if int(best_fea[i]) == 1:\n",
    "            index.append(i)\n",
    "    best_feature = []\n",
    "    for k in index:\n",
    "        best_feature.append(all_features[k])\n",
    "    X_train = df_feature[best_feature]\n",
    "    X_test = df_featuret[best_feature]\n",
    "    y_train = df_feature['label']\n",
    "    y_test = df_featuret['label']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    scores_svm_rfe = cross_val_score(clf_svm_rfe, X_train, y_train, cv=5)\n",
    "    print(F\"training score for { len(best_feature) } feature is - { scores_svm_rfe }\")\n",
    "    clf_svm_rfe.fit(X_train,y_train)\n",
    "    pred_rfe = clf_svm_rfe.predict(X_test)\n",
    "    acc_score_rfe = accuracy_score(y_test,pred_rfe)\n",
    "    print(F\"test acc_score for svm linear is - { acc_score_rfe }\")\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred_rfe)\n",
    "    TP = cm[0][0] \n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_rfe = TP / (TP + FN)\n",
    "    Specificity_rfe = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm linear is { Sensitivity_rfe }\")\n",
    "    print(F\"Specificity for svm linear is { Specificity_rfe }\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = shuffle(df_feature)\n",
    "\n",
    "X_train_full = df_feature.drop(['label'],axis=1)\n",
    "y_train_full = df_feature['label']\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train_full,y_train_full)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_train_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = []\n",
    "test_score = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "NOF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fea in [10,20,30,40,50,60]:\n",
    "    feature_importance = pd.Index.tolist(feat_importances.nlargest(fea).index)\n",
    "    fea_set = feature_importance\n",
    "    NOF.append(len(fea_set))\n",
    "    print(\"*************************************************\")\n",
    "    print(F\"features in use - \\n { fea_set }\")\n",
    "    try:\n",
    "        del X_train,clfrf,clf_svm1_lin,clf_svm_rbf,ensemble\n",
    "    except NameError as e:\n",
    "        pass\n",
    "    X_train = df_feature[fea_set]\n",
    "    y_train = df_feature['label']\n",
    "    X_test = df_featuret[fea_set]\n",
    "    y_test = df_featuret['label']\n",
    "    #scale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    #RF\n",
    "    RFC_METRIC = 'entropy'  #metric used for RandomForrestClassifier\n",
    "    NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\n",
    "    NO_JOBS = -1 #number of parallel jobs used for RandomForrestClassifier\n",
    "    clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                                 random_state=2000,\n",
    "                                 criterion=RFC_METRIC,\n",
    "                                 n_estimators=NUM_ESTIMATORS,\n",
    "                                 verbose=False)\n",
    "\n",
    "    scores_rf = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_rf)\n",
    "    print(F\"training score for RF is - { scores_rf }\")\n",
    "    clf.fit(X_train,y_train)\n",
    "    pred1 = clf.predict(X_test)\n",
    "    acc_score_rf = accuracy_score(y_test,pred1)\n",
    "    test_score.append(acc_score_rf)\n",
    "    print(F\"test acc_score for RF is - { acc_score_rf }\")\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred1)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_rf = TP / (TP + FN)\n",
    "    Specificity_rf = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for RF is { Sensitivity_rf }\")\n",
    "    print(F\"Specificity for RF is { Specificity_rf }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_rf)\n",
    "    specificity.append(Specificity_rf)\n",
    "    \n",
    "    #SVM linear\n",
    "    \n",
    "    clf_svm1 = svm.SVC(kernel='linear', C=1)\n",
    "    scores_svm1 = cross_val_score(clf_svm1, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_svm1)\n",
    "    print(F\"training score for svm linear is - { scores_svm1 }\")\n",
    "    clf_svm1.fit(X_train,y_train)\n",
    "    pred2 = clf_svm1.predict(X_test)\n",
    "    acc_score_svm1 = accuracy_score(y_test,pred2)\n",
    "    print(F\"test acc_score for svm linear is - { acc_score_svm1 }\")\n",
    "    test_score.append(acc_score_svm1)\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred2)\n",
    "    TP = cm[0][0] \n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm1 = TP / (TP + FN)\n",
    "    Specificity_svm1 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm linear is { Sensitivity_svm1 }\")\n",
    "    print(F\"Specificity for svm linear is { Specificity_svm1 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm1)\n",
    "    specificity.append(Specificity_svm1)\n",
    "    \n",
    "    #SVM rbf\n",
    "    clf_svm2 = svm.SVC(kernel='rbf', C=1)\n",
    "    scores_svm2 = cross_val_score(clf_svm2, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_svm2)\n",
    "    print(F\"training score for svm rbf is - { scores_svm2 }\")\n",
    "    clf_svm2.fit(X_train,y_train)\n",
    "    pred3 = clf_svm2.predict(X_test)\n",
    "    acc_score_svm2 = accuracy_score(y_test,pred3)\n",
    "    print(F\"test acc_score for svm rbf is - { acc_score_svm2 }\")\n",
    "    test_score.append(acc_score_svm2)\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test,pred3)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm2 = TP / (TP + FN)\n",
    "    Specificity_svm2 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm rbf is { Sensitivity_svm2 }\")\n",
    "    print(F\"Specificity for svm rbf is { Specificity_svm2 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm2)\n",
    "    specificity.append(Specificity_svm2)\n",
    "    #svm3\n",
    "    clf_svm3 = svm.SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "      decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
    "      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "      tol=0.001, verbose=False)\n",
    "    scores_svm3 = cross_val_score(clf_svm3, X_train, y_train, cv=5)\n",
    "    print(F\"training score for svm rbf is - { scores_svm3 }\")\n",
    "    training_score.append(scores_svm3)\n",
    "    clf_svm3.fit(X_train,y_train)\n",
    "    pred4 = clf_svm3.predict(X_test)\n",
    "    acc_score_svm3 = accuracy_score(y_test,pred4)\n",
    "    print(F\"test acc_score for svm rbf is - { acc_score_svm3 }\")\n",
    "    test_score.append(acc_score_svm3)\n",
    "    cm = confusion_matrix(y_test,pred4)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_svm3 = TP / (TP + FN)\n",
    "    Specificity_svm3 = TN / (TN + FP)\n",
    "\n",
    "    print(F\"Sensitivity for svm rbf is { Sensitivity_svm3 }\")\n",
    "    print(F\"Specificity for svm rbf is { Specificity_svm3 }\")\n",
    "    \n",
    "    sensitivity.append(Sensitivity_svm3)\n",
    "    specificity.append(Specificity_svm3)\n",
    "    #ensemble\n",
    "    estimators = []\n",
    "    estimators.append(('rf', clf))\n",
    "    estimators.append(('svm', clf_svm1))\n",
    "    estimators.append(('svm3', clf_svm3))\n",
    "    # create the ensemble model\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    scores_ens = cross_val_score(ensemble, X_train, y_train, cv=5)\n",
    "    training_score.append(scores_ens)\n",
    "    print(F\"score for eensemble is - { scores_ens }\")\n",
    "    ensemble.fit(X_train,y_train)\n",
    "    pred4 = ensemble.predict(X_test)\n",
    "    acc_score_ens = accuracy_score(y_test,pred4)   \n",
    "    print(F\"acc_score for ensemble is - { acc_score_ens }\")\n",
    "    test_score.append(acc_score_ens)\n",
    "    cm = confusion_matrix(y_test,pred4)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "\n",
    "    Sensitivity_ens = TP / (TP + FN)\n",
    "    Specificity_ens = TN / (TN + FP)\n",
    "    sensitivity.append(Sensitivity_ens)\n",
    "    specificity.append(Specificity_ens)\n",
    "\n",
    "    print(F\"Sensitivity for ensemble is { Sensitivity_ens }\")\n",
    "    print(F\"Specificity for ensemble is { Specificity_ens }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
